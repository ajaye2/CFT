{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7LMxiJpHnZE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bgG5AE6THnZG"
      },
      "outputs": [],
      "source": [
        "# %pip show pip\n",
        "# %pip install -U scikit-learn\n",
        "# python3 -m pip3 install jupyter notebook -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Corpus\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive \n",
        "import sys\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True) \n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/CFT\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZekWOGoDHpgE",
        "outputId": "ecb16a4a-bb92-4338-e46a-328fbf201b7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "changepoint_detection.py  LICENSE\t   README.md\t     train.py\n",
            "datasets\t\t  metafeatures.py  requirements.txt  ts2vec.ipynb\n",
            "datautils.py\t\t  models\t   scripts\t     ts2vec.py\n",
            "dl_meta_features.ipynb\t  __pycache__\t   tasks\t     utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# ## For when you're ready to save your changes...\n",
        "# git commit -a -m \"message\"\n",
        "# git push -u origin main"
      ],
      "metadata": {
        "id": "nJ5UtZI4H5WG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ReRvaPuHnZJ",
        "outputId": "69ad5772-fc12-488f-fe4c-4b57b3045b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from utils import standardize\n",
        "from datetime import datetime\n",
        "from models.losses import *\n",
        "from ts2vec import TS2Vec\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from os import walk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datautils\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EzaGzOJyHnZK"
      },
      "outputs": [],
      "source": [
        "# TODO: Challenges to solve with stock data\n",
        "    # Have to deal with weekends - time series not continuous (maybe include time features like hour and daily)\n",
        "    # Have to deal with hours between stock opened and stock closed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0YNvLHJ6HnZL"
      },
      "outputs": [],
      "source": [
        "\n",
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/\"  #\"/Users/abuj/Documents/GitHub/\n",
        "timeframe = \"hour\"\n",
        "mypath      = base_path + \"CFT/datasets/STOCKS/\" + timeframe + \"/\"\n",
        "ffd_path    = base_path + \"CFT/datasets/META_FEATURES/FFD/\" + timeframe + \"/\"\n",
        "cpd_path    = base_path + \"CFT/datasets/META_FEATURES/CPD/\" + timeframe + \"/\"\n",
        "\n",
        "filenames   = next(walk( mypath), (None, None, []))[2]  \n",
        "\n",
        "cols_to_perform_ffd = ['open', 'high', 'low', 'close', 'vwap']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "short_cpd_lookback_window_length    = 12\n",
        "long_cpd_lookback_window_length     = 126"
      ],
      "metadata": {
        "id": "bKp2GjQT-B1G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPA5L-Fp_xPy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmq19eXOHnZL",
        "outputId": "e7984809-666d-4197-d342-43b510c84ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:38<00:00,  5.44s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\n",
        "data_dict = {}\n",
        "\n",
        "LOOKBACK_WINDOW                 = 7      # Hyper parameter\n",
        "STANDARIZE_LOOKBACK_WINDOW      = 7 * 3  # Hyper parameter\n",
        "\n",
        "num_assets = 0\n",
        "for file in tqdm(filenames[330:]):\n",
        "    df                  = pd.read_csv(mypath + file).set_index('timestamp')[cols_to_perform_ffd]\n",
        "    df_ffd              = pd.read_csv(ffd_path + file).set_index('timestamp')\n",
        "\n",
        "    if not Path(cpd_path + file[:-4] + \"_short.csv\").is_file() or not Path(cpd_path + file[:-4] + \"_long.csv\").is_file():  continue \n",
        "    df_cpd_short        = pd.read_csv(cpd_path + file[:-4] + \"_short.csv\").set_index('date').drop(['t', 'cp_location'], axis=1).fillna(method='ffill')\n",
        "    df_cpd_long         = pd.read_csv(cpd_path + file[:-4] + \"_long.csv\").set_index('date').drop(['t', 'cp_location'], axis=1).fillna(method='ffill')\n",
        "\n",
        "    df_cpd_long.index     = df[-df_cpd_long.shape[0]:].index \n",
        "    df_cpd_short.index    = df[-df_cpd_short.shape[0]:].index \n",
        "\n",
        "    df_cpd_long.columns   = [x + \"_long\" for x in df_cpd_long.columns]\n",
        "    df_cpd_short.columns  = [x + \"_short\" for x in df_cpd_short.columns]\n",
        "\n",
        "    meta_features         = pd.concat([df_ffd, df_cpd_short, df_cpd_long], axis=1).sort_index().dropna()\n",
        "\n",
        "\n",
        "\n",
        "    vol                                   = df.close.ewm(halflife=1).std()\n",
        "    \n",
        "    tasks                                 = pd.DataFrame(index=df.index)\n",
        "    tasks['one_day_price_pred']           = ( df.close - df.close.shift(-1) )\n",
        "    tasks['two_day_price_pred']           = ( df.close - df.close.shift(-2) )\n",
        "    tasks['three_day_price_pred']         = ( df.close - df.close.shift(-3) )\n",
        "    tasks['four_day_price_pred']          = ( df.close - df.close.shift(-4) )\n",
        "    tasks['five_day_price_pred']          = ( df.close - df.close.shift(-5) )\n",
        "\n",
        "    ### Check if volatilty is being created correctly\n",
        "    tasks['one_week_vol_pred']            = ( df.close.rolling(5).std() - df.close.rolling(5).std().shift(-1) )\n",
        "    tasks['one_month_vol_pred']           = ( df.close.rolling(21).std() - df.close.rolling(21).std().shift(-1) )\n",
        "\n",
        "    tasks['one_week_skew_pred']            = ( df.close.rolling(5).skew() - df.close.rolling(5).skew().shift(-1) )\n",
        "    tasks['one_month_skew_pred']           = ( df.close.rolling(21).skew() - df.close.rolling(21).skew().shift(-1) )\n",
        "\n",
        "    # tasks.dropna(inplace=True)\n",
        "\n",
        "    # Experiment between this and using raw values \n",
        "    df                  = standardize(df,     look_back=STANDARIZE_LOOKBACK_WINDOW)\n",
        "    meta_features       = standardize(meta_features, look_back=STANDARIZE_LOOKBACK_WINDOW)\n",
        "\n",
        "    idx                 = meta_features.index.intersection(df.index).intersection(tasks.index)\n",
        "    df, meta_features,tasks    = df.loc[idx], meta_features.loc[idx], tasks.loc[idx]\n",
        "\n",
        "\n",
        "    X_data_array, X_data_dict                = [], {}       \n",
        "    EXP_FEAT_data_array                      = []\n",
        "\n",
        "\n",
        "    for i in range(LOOKBACK_WINDOW, len(df)+1):\n",
        "        X_data_array.append( df.iloc[i - LOOKBACK_WINDOW:i].values )\n",
        "        EXP_FEAT_data_array.append( meta_features.iloc[i-1].values ) # Without minus one, the exp features lead X_DATA by one tiemstamp\n",
        "        X_data_dict[df.index[i-1]]  = df.iloc[i - LOOKBACK_WINDOW:i].values\n",
        "        # EXP_FEAT_data_array[meta_features.index[i-1]] = meta_features.iloc[i-1].values\n",
        "    \n",
        "\n",
        "        assert meta_features.iloc[i-1].name == df.iloc[i - LOOKBACK_WINDOW:i].iloc[-1].name \n",
        "\n",
        "    # X_data_array        = np.array( X_data_array )\n",
        "    # EXP_FEAT_data_array = np.array( EXP_FEAT_data_array )\n",
        "\n",
        "    data_dict[file[:-4]]                  = {}\n",
        "    data_dict[file[:-4]]['X_DATA']        = X_data_array\n",
        "    data_dict[file[:-4]]['EXP_FEAT_DATA'] = EXP_FEAT_data_array\n",
        "    data_dict[file[:-4]]['Y_DATA']        = tasks \n",
        "    data_dict[file[:-4]]['X_DATA_DICT']   = X_data_dict \n",
        "    # print(file, X_data_array.shape, EXP_FEAT_data_array.shape)\n",
        "\n",
        "    num_assets+=1\n",
        "num_assets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90TbLfn-HnZM",
        "outputId": "5c1ac04e-9a55-494f-9a1c-8e9fae9c8aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48444, 7, 5) (48444, 14)\n",
            "(20764, 7, 5) (20764, 14)\n"
          ]
        }
      ],
      "source": [
        "pct_train          = 0.7\n",
        "train_data         = np.concatenate( [data_dict[x]['X_DATA'][:int(pct_train*len(data_dict[x]['X_DATA']))] for x in data_dict.keys()])\n",
        "exp_train_data     = np.concatenate( [data_dict[x]['EXP_FEAT_DATA'][:int(pct_train*len(data_dict[x]['X_DATA']))] for x in data_dict.keys()])\n",
        "# train_labels       = np.concatenate( [data_dict[x]['Y_DATA'][:int(pct_train*len(data_dict[x]['X_DATA']))] for x in data_dict.keys()])\n",
        "assert train_data.shape[0] == exp_train_data.shape[0] \n",
        "print(train_data.shape, exp_train_data.shape)\n",
        "\n",
        "\n",
        "test_data         = np.concatenate( [data_dict[x]['X_DATA'][int(pct_train*len(data_dict[x]['X_DATA'])):] for x in data_dict.keys()])\n",
        "exp_test_data     = np.concatenate( [data_dict[x]['EXP_FEAT_DATA'][int(pct_train*len(data_dict[x]['X_DATA'])):] for x in data_dict.keys()])\n",
        "# test_labels       = np.concatenate( [data_dict[x]['Y_DATA'][:int(pct_train*len(data_dict[x]['X_DATA']))] for x in data_dict.keys()])\n",
        "assert test_data.shape[0] == exp_test_data.shape[0] \n",
        "print(test_data.shape, exp_test_data.shape)\n",
        "# # (Both train_data and test_data have a shape of n_instances x n_timestamps x n_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "NxvZbrcrHnZN",
        "outputId": "f92340a9-c769-46d5-a425-205d3cde15a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4e495be49942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m loss_log = model.fit(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mexpert_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# train_data.reshape(100, -1)[:,40:],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/1I-TlHPlFRflludAocR05-w2iazYP1z-o/CFT/ts2vec.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, expert_features, n_epochs, n_iters, verbose, temperature, delta, loss_weight_scale, use_expclr_loss)\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0muse_expclr_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpclr_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                         \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpclr_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquadratic_contrastive_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/1I-TlHPlFRflludAocR05-w2iazYP1z-o/CFT/models/losses.py\u001b[0m in \u001b[0;36mexpclr_loss\u001b[0;34m(z1, f, temp, delta, type_sim)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mlij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_quad_loss_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdij_mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_diff_norm_of_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_sim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mlij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlij\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;31m# lij /= B**2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Train a TS2Vec model\n",
        "model = TS2Vec(\n",
        "    input_dims=train_data.shape[-1],\n",
        "    device=0,\n",
        "    output_dims=100,\n",
        "    batch_size=64, \n",
        "   \n",
        ")\n",
        "loss_log = model.fit(\n",
        "    train_data,\n",
        "    expert_features=exp_train_data, # train_data.reshape(100, -1)[:,40:],\n",
        "    verbose=True,\n",
        "    use_expclr_loss=True,\n",
        "    n_epochs=10\n",
        ")\n",
        "\n",
        "# # Compute timestamp-level representations for test set\n",
        "# test_repr_tl = model.encode(test_data)  # n_instances x n_timestamps x output_dims\n",
        "\n",
        "# Compute instance-level representations for test set\n",
        "# test_repr_il = model.encode(test_data, encoding_window='full_series')  # n_instances x output_dims\n",
        "\n",
        "# # Sliding inference for test set\n",
        "# test_repr_si = model.encode(\n",
        "#     test_data,\n",
        "#     casual=True,\n",
        "#     sliding_length=1,\n",
        "#     sliding_padding=50\n",
        "# )  # n_instances x n_timestamps x output_dims\n",
        "# # (The timestamp t's representation vector is computed using the observations located in [t-50, t])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlHQRshhGBav",
        "outputId": "bb86c608-8e5b-4cb3-c50d-3251c00a6665"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['XEL', 'XRAY', 'XRX', 'XYL', 'ZION'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "tw6TlDA4HnZN",
        "outputId": "4e6a6545-6117-45f7-818f-797a8f7fe6fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-747273fa46bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0marrar_for_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0marrar_for_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEST_ASSET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_DATA_DICT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full_series'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtest_repr_il\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrar_for_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Timestamp('2020-08-06 16:00:00+0000', tz='UTC')"
          ]
        }
      ],
      "source": [
        "# data_dict[file[:-4]]['X_DATA_DICT']\n",
        "TEST_ASSET = \"XEL\"\n",
        "\n",
        "num_test_intances = int(pct_train*len(data_dict[TEST_ASSET]['Y_DATA']))\n",
        "\n",
        "test_y = data_dict[TEST_ASSET]['Y_DATA'][num_test_intances:].dropna().iloc[:1000]\n",
        "\n",
        "test_x = { idx: data_dict[TEST_ASSET]['X_DATA_DICT'][idx]  for idx in test_y.index if idx in data_dict[TEST_ASSET]['X_DATA_DICT'].keys() }\n",
        "\n",
        "print( len(test_x) )\n",
        "print( len(test_y) )\n",
        "\n",
        "arrar_for_df = []\n",
        "for idx in test_y.index[:]:\n",
        "    arrar_for_df.append( model.encode(data_dict[TEST_ASSET]['X_DATA_DICT'][idx][np.newaxis, ...], encoding_window='full_series').reshape(-1) )\n",
        "\n",
        "test_repr_il = pd.DataFrame(arrar_for_df, index = test_y.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUr0eCllHnZN",
        "outputId": "f3bd458a-0402-469f-97b2-8dd0c60fcde3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.573134328358209\n",
            "0.41818181818181815\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(test_repr_il, test_y, test_size=0.33, random_state=42)\n",
        "reg = linear_model.LinearRegression()\n",
        "reg = RidgeClassifier()\n",
        "\n",
        "\n",
        "reg.fit( X_train, np.sign( y_train['one_day_price_pred'] ) )\n",
        "\n",
        "print( reg.score(X_train, np.sign( y_train['one_day_price_pred'] ) ) )\n",
        "print( reg.score(X_test,  np.sign( y_test['one_day_price_pred'] ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfwJe6mnHnZO",
        "outputId": "ff445128-9c8e-497e-ad1f-7b0aa542fea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " XEL\n",
            "full_series\n",
            "0.5373134328358209\n",
            "0.509090909090909\n",
            "net_compression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.491044776119403\n",
            "0.4303030303030303\n",
            "\n",
            " XRAY\n",
            "full_series\n",
            "0.5686567164179105\n",
            "0.4954545454545455\n",
            "net_compression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5313432835820896\n",
            "0.47878787878787876\n",
            "\n",
            " XRX\n",
            "full_series\n",
            "0.5462686567164179\n",
            "0.5060606060606061\n",
            "net_compression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5156716417910447\n",
            "0.4666666666666667\n",
            "\n",
            " XYL\n",
            "full_series\n",
            "0.5658986175115207\n",
            "0.4803738317757009\n",
            "net_compression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5225806451612903\n",
            "0.45981308411214955\n",
            "\n",
            " ZION\n",
            "full_series\n",
            "0.5417910447761194\n",
            "0.45606060606060606\n",
            "net_compression\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5171641791044777\n",
            "0.46060606060606063\n",
            "\n",
            " net_compression acheives better results  0.2 % of the times \n"
          ]
        }
      ],
      "source": [
        "# data_dict[file[:-4]]['X_DATA_DICT']  \n",
        "#  \n",
        "\n",
        "results = {}\n",
        "\n",
        "mean = 0\n",
        "\n",
        "for TEST_ASSET in  list( data_dict.keys() )[:]:\n",
        "    print(\"\\n\", TEST_ASSET)\n",
        "    results[TEST_ASSET] = {}\n",
        "    \n",
        "    num_test_intances = int(pct_train*len(data_dict[TEST_ASSET]['Y_DATA']))\n",
        "    x_data_index = pd.DatetimeIndex( data_dict[TEST_ASSET]['X_DATA_DICT'].keys() )\n",
        "    data_dict[TEST_ASSET]['Y_DATA'].index  = pd.to_datetime( data_dict[TEST_ASSET]['Y_DATA'].index )\n",
        "\n",
        "    idx = data_dict[TEST_ASSET]['Y_DATA'].dropna().index.intersection(x_data_index)\n",
        "    test_y = data_dict[TEST_ASSET]['Y_DATA'][:].loc[idx].iloc[-2000:] #.dropna()\n",
        "\n",
        "    test_x = { pd.to_datetime( idx ): val for idx, val in data_dict[TEST_ASSET]['X_DATA_DICT'].items() }\n",
        "\n",
        "    # print( len(test_x) )\n",
        "    # print( len(test_y) )\n",
        "\n",
        "    encoding_window_method = ['full_series', 'net_compression']\n",
        "    test_name = 'one_day_price_pred'\n",
        "    for encoding_window in encoding_window_method:\n",
        "        print(encoding_window)\n",
        "        arrar_for_df = [ model.encode(test_x[idx][np.newaxis, ...], encoding_window=encoding_window).reshape(-1) for idx in test_y.index ]\n",
        "\n",
        "        test_repr_il = pd.DataFrame(arrar_for_df, index = test_y.index)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(test_repr_il, test_y, test_size=0.33, random_state=42)\n",
        "        # reg = linear_model.LinearRegression()\n",
        "        reg = RidgeClassifier()\n",
        "        # reg = linear_model.Lasso(alpha=0.1)\n",
        "\n",
        "        clf = ExtraTreesClassifier(n_estimators=50)\n",
        "        clf = clf.fit(X_train, np.sign( y_train[test_name] ))\n",
        "        feat_selec_model = SelectFromModel(clf, prefit=True)\n",
        "        X_train = feat_selec_model.transform(X_train)\n",
        "        X_test = feat_selec_model.transform(X_test)\n",
        "\n",
        "        reg.fit( X_train, np.sign( y_train[test_name] ) )\n",
        "\n",
        "        results[TEST_ASSET][encoding_window] = reg.score(X_test,  np.sign( y_test[test_name] ) )\n",
        "        \n",
        "        print( reg.score(X_train, np.sign( y_train[test_name] ) ) )\n",
        "        print(results[TEST_ASSET][encoding_window])\n",
        "        # print( reg.score(X_test,  np.sign( y_test[test_name] ) ) )\n",
        "\n",
        "    # print(results[TEST_ASSET]['full_series'], results[TEST_ASSET]['net_compression'] )\n",
        "        \n",
        "    mean += results[TEST_ASSET]['full_series'] < results[TEST_ASSET]['net_compression']\n",
        "\n",
        "mean /= len(results.keys())\n",
        "\n",
        "\n",
        "print( \"\\n net_compression acheives better results \", mean, \"% of the times \" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQttm0AHHnZO"
      },
      "outputs": [],
      "source": [
        "idx = np.sign( y_test['one_day_price_pred'] )[ np.sign( y_test['one_day_price_pred'] ) != 0].index\n",
        "for i in range(100):\n",
        "    acc = ( np.sign( X_test )[i].loc[idx] == np.sign( y_test['one_day_price_pred'].loc[idx] ) ).mean()\n",
        "    if acc > .52:\n",
        "        print(i, acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVSoLbdoHnZP"
      },
      "outputs": [],
      "source": [
        "test_repr_il[2].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doCvbWbBHnZP"
      },
      "outputs": [],
      "source": [
        "results[TEST_ASSET][encoding_window]\n",
        "\n",
        "# TODO: Result has same values for all keys\n",
        "mean = 0\n",
        "for key in results.keys():\n",
        "    print(key, results[TEST_ASSET]['full_series'], results[TEST_ASSET]['net_compression'])\n",
        "    mean += results[TEST_ASSET]['full_series'] < results[TEST_ASSET]['net_compression']\n",
        "\n",
        "mean /= len(results.keys())\n",
        "mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAorT_KpHnZP"
      },
      "outputs": [],
      "source": [
        "test_repr_il[0].plot()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2WAGqtMBVLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GK00b9XKBVIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3RXz_wnVBVEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZU7mS0GCBVB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_lvW9lRBU_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKnpDiaAHnZP"
      },
      "outputs": [],
      "source": [
        "### TEST LOSS FUNCTIONS\n",
        "BATCH_SIZE              = 100\n",
        "rnn                     = nn.LSTM(5, 20, batch_first=True).float()\n",
        "loss_test_values        = torch.from_numpy( train_data[:BATCH_SIZE] ).float()\n",
        "output, (hn, cn)        = rnn(loss_test_values)\n",
        "hn                      = hn.squeeze(0)\n",
        "loss_test_exp_values    = torch.from_numpy( exp_train_data[:BATCH_SIZE]).float()\n",
        "\n",
        "print( \"quadratic_contrastive_loss: \", quadratic_contrastive_loss(hn, loss_test_exp_values, delta=1) )\n",
        "print( \"expclr_loss: \", expclr_loss(hn, loss_test_exp_values, delta=1) )\n",
        "\n",
        "# max_diff = get_max_norm(batch_f)\n",
        "# similarity_measure(batch_f[0], batch_f[1], max_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-iToIyeHnZQ"
      },
      "outputs": [],
      "source": [
        "# out = model._eval_with_pooling(torch.from_numpy( test_data[:64] ).to(torch.float), None, encoding_window='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfTiW3EBHnZQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # # Load the ECG200 dataset from UCR archive\n",
        "# train_data, train_labels, test_data, test_labels = datautils.load_ECG()\n",
        "# # # (Both train_data and test_data have a shape of n_instances x n_timestamps x n_features)\n",
        "\n",
        "# exp_feat = pd.DataFrame(  train_data.reshape(100, -1)[:,40:] )\n",
        "# exp_feat = pd.DataFrame(  exp_feat.mean(axis=1) ).values\n",
        "\n",
        "\n",
        "# # Train a TS2Vec model\n",
        "# model = TS2Vec(\n",
        "#     input_dims=1,\n",
        "#     device=0,\n",
        "#     output_dims=320\n",
        "# )\n",
        "# loss_log = model.fit(\n",
        "#     train_data,\n",
        "#     expert_features=exp_feat, #train_data.reshape(100, -1)[:,40:],\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# # Compute timestamp-level representations for test set\n",
        "# test_repr_tl = model.encode(test_data)  # n_instances x n_timestamps x output_dims\n",
        "\n",
        "# # Compute instance-level representations for test set\n",
        "# test_repr_il = model.encode(test_data, encoding_window='full_series')  # n_instances x output_dims\n",
        "\n",
        "# # Sliding inference for test set\n",
        "# test_repr_si = model.encode(\n",
        "#     test_data,\n",
        "#     casual=True,\n",
        "#     sliding_length=1,\n",
        "#     sliding_padding=50\n",
        "# )  # n_instances x n_timestamps x output_dims\n",
        "# # (The timestamp t's representation vector is computed using the observations located in [t-50, t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPC5GCUXHnZR"
      },
      "outputs": [],
      "source": [
        "# data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols = datautils.load_forecast_csv(\"ETTh2\")\n",
        "# train_data = data[:, train_slice]\n",
        "        \n",
        "# # Train a TS2Vec model\n",
        "# model = TS2Vec(\n",
        "#     input_dims=1,\n",
        "#     device=0,\n",
        "#     output_dims=320\n",
        "# )\n",
        "# loss_log = model.fit(\n",
        "#     train_data,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# # Compute timestamp-level representations for test set\n",
        "# test_repr = model.encode(test_data)  # n_instances x n_timestamps x output_dims\n",
        "\n",
        "# # Compute instance-level representations for test set\n",
        "# test_repr = model.encode(test_data, encoding_window='full_series')  # n_instances x output_dims\n",
        "\n",
        "# # Sliding inference for test set\n",
        "# test_repr = model.encode(\n",
        "#     test_data,\n",
        "#     casual=True,\n",
        "#     sliding_length=1,\n",
        "#     sliding_padding=50\n",
        "# )  # n_instances x n_timestamps x output_dims\n",
        "# # (The timestamp t's representation vector is computed using the observations located in [t-50, t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPYVSTEqHnZR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}