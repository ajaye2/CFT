{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from matplotlib import pyplot as plt\n",
    "from changepoint_detection import *\n",
    "from utils import standardize\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from models.losses import *\n",
    "from ts2vec import TS2Vec\n",
    "from tqdm import tqdm\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datautils\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from polygon import RESTClient\n",
    "from metafeatures import *\n",
    "\n",
    " \n",
    "\n",
    "USE_KM_HYP_TO_INITIALISE_KC = True\n",
    "\n",
    "# data_url    = 'https://datahub.io/core/s-and-p-500-companies/datapackage.json'\n",
    "# f           = open(\"/Users/abuj/Documents/GitHub/CFT/datasets/sp500_constituents.json\")\n",
    "# data        = json.load(f)\n",
    "# # len( data['2012/12/31'] )\n",
    "\n",
    "\n",
    "print ( os.cpu_count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH                           = \"/Users/abuj/Documents/GitHub/CFT/datasets/\" #\"/home/ec2-user/CFT/datasets/\"\n",
    "stock_data_path_hour                = BASE_PATH + \"STOCKS/hour/\"\n",
    "ffd_path_hour                       = BASE_PATH + \"META_FEATURES/FFD/hour/\"\n",
    "cpd_path_hour                       = BASE_PATH + \"META_FEATURES/CPD/hour/\"\n",
    "\n",
    "stock_data_path_day                 = BASE_PATH + \"STOCKS/day/\"\n",
    "ffd_path_day                        = BASE_PATH + \"META_FEATURES/FFD/day/\"\n",
    "cpd_path_day                        = BASE_PATH + \"META_FEATURES/CPD/day/\"\n",
    "\n",
    "cpd_path_test                       = BASE_PATH + \"META_FEATURES/CPD/test/\"\n",
    "cpd_path_test2                      = BASE_PATH + \"META_FEATURES/CPD/test2/\"\n",
    "\n",
    "short_cpd_lookback_window_length    = 12\n",
    "long_cpd_lookback_window_length     = 126\n",
    "\n",
    "filenames_hour                      = next(walk(stock_data_path_hour), (None, None, []))[2] \n",
    "filenames_day                       = next(walk(stock_data_path_day), (None, None, []))[2]  \n",
    "\n",
    "cols_to_perform_ffd = ['open', 'high', 'low', 'close', 'vwap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/337 [00:03<09:32,  1.71s/it]"
     ]
    }
   ],
   "source": [
    "'''Download data from polygon'''\n",
    "\n",
    "# client = RESTClient(\"ZV6Xy3SWzsJ2xawzRp0JXlmUbbIHePWF\")\n",
    "# START_DATE  = pd.to_datetime(\"2012-12-31\")\n",
    "# END_DATE    = pd.to_datetime(\"2022-10-09\")\n",
    "\n",
    "# with open(\"/Users/abuj/Documents/GitHub/CFT/datasets/ASSETS_NOT_SUPPORTED_BY_POLYGON\", \"rb\") as fp:\n",
    "#    ASSETS_NOT_SUPPORTED_BY_POLYGON = pickle.load(fp)\n",
    "\n",
    "# ASSETS = data['2012/12/31']\n",
    "# ASSETS = set(ASSETS) - set(ASSETS_NOT_SUPPORTED_BY_POLYGON)\n",
    "# ASSETS = list( ASSETS )\n",
    "\n",
    "# aggregates = ['day', 'hour']\n",
    "# time_delta = {}\n",
    "# time_delta['day']  = pd.Timedelta(days=1500)\n",
    "# time_delta['hour'] = pd.Timedelta(hours=800)\n",
    "\n",
    "# failed_assets = []\n",
    "# stocks_folder = \"/Users/abuj/Documents/GitHub/CFT/datasets/STOCKS/\"\n",
    "# for timeframe in aggregates:\n",
    "#     for x in tqdm( ASSETS[:] ):\n",
    "#         if x in ASSETS_NOT_SUPPORTED_BY_POLYGON: continue\n",
    "#         try:\n",
    "#             file_path = stocks_folder + timeframe + \"/\" + x + \".csv\"\n",
    "#             my_file = Path(file_path)\n",
    "#             if my_file.is_file(): continue\n",
    "            \n",
    "#             curr_date = START_DATE\n",
    "#             aggs = []\n",
    "#             while curr_date < END_DATE:\n",
    "#                 until_date = curr_date + time_delta[timeframe]\n",
    "#                 until_date = min(until_date, END_DATE)\n",
    "#                 if (until_date - curr_date).days <= 0:\n",
    "#                     break\n",
    "#                 # print(curr_date, until_date)\n",
    "#                 aggs += client.get_aggs(x, 1, timeframe, curr_date.strftime(\"%Y-%m-%d\"), until_date.strftime(\"%Y-%m-%d\"), limit=50000)\n",
    "#                 curr_date += time_delta[timeframe]\n",
    "\n",
    "#             df = pd.DataFrame(aggs).drop('otc', axis=1)\n",
    "#             df.timestamp = pd.to_datetime(df.timestamp, unit='ms', utc=True)\n",
    "#             df.drop_duplicates(inplace=True)\n",
    "#             df.set_index('timestamp', inplace=True)\n",
    "\n",
    "#             df.to_csv(file_path)\n",
    "#         except:\n",
    "#             print(x, \" failed\")\n",
    "#             failed_assets.append(x)\n",
    "\n",
    "\n",
    "# li = list( set( failed_assets + ASSETS_NOT_SUPPORTED_BY_POLYGON ) )\n",
    "# with open(\"/Users/abuj/Documents/GitHub/CFT/datasets/ASSETS_NOT_SUPPORTED_BY_POLYGON\", \"wb\") as fp:   #Pickling\n",
    "#    pickle.dump(li, fp)\n",
    "\n",
    "'''Process FFD data from polygon'''\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "for file in tqdm(filenames_day[:]):\n",
    "    try:\n",
    "        file_path = ffd_path_day + \"/\" + file\n",
    "        my_file = Path(file_path)\n",
    "        # if my_file.is_file(): continue\n",
    "        \n",
    "        df = pd.read_csv( stock_data_path_day + file , index_col='timestamp')\n",
    "        df.index  = pd.to_datetime( df.index )\n",
    "        df_ffd = pd.DataFrame(index=df.index)\n",
    "\n",
    "        for col in cols_to_perform_ffd:\n",
    "            df_ffd[col+\"_0.3\"]  = get_frac_diff_series(df, col, coef_d=0.3, plot=False)\n",
    "            df_ffd[col+\"_0.6\"]  = get_frac_diff_series(df, col, coef_d=0.6, plot=False)\n",
    "\n",
    "        df_ffd = df_ffd.dropna(axis=1, thresh=int(0.6*df_ffd.shape[0]))\n",
    "\n",
    "        max_nans            = df_ffd.isna().sum().max() + 1\n",
    "        df_ffd              = df_ffd[max_nans:]\n",
    "\n",
    "        if df_ffd.shape[0] < 1800:\n",
    "            print(file, df_ffd.shape)\n",
    "            \n",
    "        df_ffd.to_csv(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e, file, \" failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02 05:00:00+00:00</th>\n",
       "      <td>0.022961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03 05:00:00+00:00</th>\n",
       "      <td>0.003582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04 05:00:00+00:00</th>\n",
       "      <td>0.019748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07 05:00:00+00:00</th>\n",
       "      <td>-0.007235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08 05:00:00+00:00</th>\n",
       "      <td>-0.007990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           daily_returns\n",
       "date                                    \n",
       "2013-01-02 05:00:00+00:00       0.022961\n",
       "2013-01-03 05:00:00+00:00       0.003582\n",
       "2013-01-04 05:00:00+00:00       0.019748\n",
       "2013-01-07 05:00:00+00:00      -0.007235\n",
       "2013-01-08 05:00:00+00:00      -0.007990"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def prep_data_for_cpd(file, folder_path):\n",
    "\n",
    "    temp            = pd.read_csv(folder_path + file).set_index('timestamp')[cols_to_perform_ffd]\n",
    "    temp.index      = pd.to_datetime(temp.index)\n",
    "    temp            = temp[['close']].pct_change().reset_index().rename({'timestamp': 'date', 'close': 'daily_returns'}, axis=1).dropna()\n",
    "    temp            = temp.set_index('date')\n",
    "    \n",
    "    return temp\n",
    "prep_data_for_cpd(filenames_day[0], stock_data_path_day).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookback_window_length = 24\n",
    "# # ~10 mins to run 750 points (or 2 years at daily granularity) 30 mins to finish daily\n",
    "# run_module(\n",
    "#     temp, lookback_window_length, \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/CPD/day/A.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# files = filenames_day[:]\n",
    "# cpd_args_day_short = [( prep_data_for_cpd(x, stock_data_path_day),  short_cpd_lookback_window_length, cpd_path_day  + x[:-4] + \"_short.csv\" ) for x in files]\n",
    "# cpd_args_day_long  = [( prep_data_for_cpd(x, stock_data_path_day),  long_cpd_lookback_window_length,  cpd_path_day  + x[:-4] + \"_long.csv\"  ) for x in files]\n",
    "\n",
    "# with Pool(48) as pool:\n",
    "#     results = pool.starmap(run_module, cpd_args_day_short)\n",
    "#     results = pool.starmap(run_module, cpd_args_day_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = filenames_hour[:]\n",
    "# cpd_args_hour_short = [( prep_data_for_cpd(x, stock_data_path_hour),  short_cpd_lookback_window_length, cpd_path_hour  + x[:-4] + \"_short.csv\"  ) for x in files]\n",
    "# cpd_args_hour_long  = [( prep_data_for_cpd(x, stock_data_path_hour),  short_cpd_lookback_window_length, cpd_path_hour  + x[:-4] + \"_long.csv\"   ) for x in files]\n",
    "\n",
    "# with Pool() as pool:\n",
    "#     results = pool.starmap(run_module, cpd_args_hour_short)\n",
    "#     results = pool.starmap(run_module, cpd_args_hour_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_consistency_between_two_dataframes(df1, df2):\n",
    "\n",
    "    #TODO Add more verification\n",
    "\n",
    "    for col in df1.columns: assert col in df2.columns\n",
    "    assert df1.shape == df2.shape\n",
    "\n",
    "    for col in df1:\n",
    "        print((df1[col] - df2[col]).mean())\n",
    "        assert (df1[col] - df2[col]).mean() == 0\n",
    "\n",
    "    print(\"The two DataFrames are the same\")\n",
    "\n",
    "    pass\n",
    "\n",
    "df1 = pd.read_csv( cpd_path_test2 +\"/CSCO.csv\" ).set_index('date')\n",
    "\n",
    "df2 = pd.read_csv( cpd_path_day+\"/CSCO.csv\" ).set_index('date')\n",
    "(abs(df1 - df2) > 1e-4).mean()\n",
    "(df1 - df2).cp_location_norm.plot()\n",
    "check_for_consistency_between_two_dataframes(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.689471\n",
       "1       0.548039\n",
       "2       0.834246\n",
       "3       0.819757\n",
       "4       0.917409\n",
       "          ...   \n",
       "2437    0.519940\n",
       "2438    0.605042\n",
       "2439    0.083087\n",
       "2440    0.627954\n",
       "2441    0.473023\n",
       "Name: cp_location_norm, Length: 2442, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv( \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/CPD/A_temp_cpd.csv\" ).cp_location_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
