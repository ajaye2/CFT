{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abuj/Documents/GitHub/CFT/env/lib/python3.9/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.decorator.check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/Users/abuj/Documents/GitHub/CFT/env/lib/python3.9/site-packages/gpflow/experimental/utils.py:42: UserWarning: You're calling gpflow.experimental.check_shapes.inheritance.inherit_check_shapes which is considered *experimental*. Expect: breaking changes, poor documentation, and bugs.\n",
      "  warn(\n",
      "/Users/abuj/Documents/GitHub/CFT/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from matplotlib import pyplot as plt\n",
    "from changepoint_detection import *\n",
    "from utils import standardize\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from models.losses import *\n",
    "from ts2vec import TS2Vec\n",
    "from tqdm import tqdm\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datautils\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from polygon import RESTClient\n",
    "# from metafeatures import *\n",
    "\n",
    " \n",
    "\n",
    "USE_KM_HYP_TO_INITIALISE_KC = True\n",
    "\n",
    "data_url    = 'https://datahub.io/core/s-and-p-500-companies/datapackage.json'\n",
    "f           = open(\"/Users/abuj/Documents/GitHub/CFT/datasets/sp500_constituents.json\")\n",
    "data        = json.load(f)\n",
    "# len( data['2012/12/31'] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath      = \"/Users/abuj/Documents/GitHub/CFT/datasets/STOCKS/hour/\"\n",
    "ffd_path    = \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/FFD/hour/\"\n",
    "cpd_path    = \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/CPD/hour/\"\n",
    "\n",
    "filenames   = next(walk(mypath), (None, None, []))[2]  \n",
    "\n",
    "cols_to_perform_ffd = ['open', 'high', 'low', 'close', 'vwap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Download data from polygon'''\n",
    "\n",
    "# client = RESTClient(\"ZV6Xy3SWzsJ2xawzRp0JXlmUbbIHePWF\")\n",
    "# START_DATE  = pd.to_datetime(\"2012-12-31\")\n",
    "# END_DATE    = pd.to_datetime(\"2022-10-09\")\n",
    "\n",
    "# with open(\"/Users/abuj/Documents/GitHub/CFT/datasets/ASSETS_NOT_SUPPORTED_BY_POLYGON\", \"rb\") as fp:\n",
    "#    ASSETS_NOT_SUPPORTED_BY_POLYGON = pickle.load(fp)\n",
    "\n",
    "# ASSETS = data['2012/12/31']\n",
    "# ASSETS = set(ASSETS) - set(ASSETS_NOT_SUPPORTED_BY_POLYGON)\n",
    "# ASSETS = list( ASSETS )\n",
    "\n",
    "# aggregates = ['day', 'hour']\n",
    "# time_delta = {}\n",
    "# time_delta['day']  = pd.Timedelta(days=1500)\n",
    "# time_delta['hour'] = pd.Timedelta(hours=800)\n",
    "\n",
    "# failed_assets = []\n",
    "# stocks_folder = \"/Users/abuj/Documents/GitHub/CFT/datasets/STOCKS/\"\n",
    "# for timeframe in aggregates:\n",
    "#     for x in tqdm( ASSETS[:] ):\n",
    "#         if x in ASSETS_NOT_SUPPORTED_BY_POLYGON: continue\n",
    "#         try:\n",
    "#             file_path = stocks_folder + timeframe + \"/\" + x + \".csv\"\n",
    "#             my_file = Path(file_path)\n",
    "#             if my_file.is_file(): continue\n",
    "            \n",
    "#             curr_date = START_DATE\n",
    "#             aggs = []\n",
    "#             while curr_date < END_DATE:\n",
    "#                 until_date = curr_date + time_delta[timeframe]\n",
    "#                 until_date = min(until_date, END_DATE)\n",
    "#                 if (until_date - curr_date).days <= 0:\n",
    "#                     break\n",
    "#                 # print(curr_date, until_date)\n",
    "#                 aggs += client.get_aggs(x, 1, timeframe, curr_date.strftime(\"%Y-%m-%d\"), until_date.strftime(\"%Y-%m-%d\"), limit=50000)\n",
    "#                 curr_date += time_delta[timeframe]\n",
    "\n",
    "#             df = pd.DataFrame(aggs).drop('otc', axis=1)\n",
    "#             df.timestamp = pd.to_datetime(df.timestamp, unit='ms', utc=True)\n",
    "#             df.drop_duplicates(inplace=True)\n",
    "#             df.set_index('timestamp', inplace=True)\n",
    "\n",
    "#             df.to_csv(file_path)\n",
    "#         except:\n",
    "#             print(x, \" failed\")\n",
    "#             failed_assets.append(x)\n",
    "\n",
    "\n",
    "# li = list( set( failed_assets + ASSETS_NOT_SUPPORTED_BY_POLYGON ) )\n",
    "# with open(\"/Users/abuj/Documents/GitHub/CFT/datasets/ASSETS_NOT_SUPPORTED_BY_POLYGON\", \"wb\") as fp:   #Pickling\n",
    "#    pickle.dump(li, fp)\n",
    "\n",
    "'''Process FFD data from polygon'''\n",
    "\n",
    "# for file in tqdm(filenames):\n",
    "#     try:\n",
    "#         file_path = ffd_path + \"/\" + file\n",
    "#         my_file = Path(file_path)\n",
    "#         if my_file.is_file(): continue\n",
    "        \n",
    "#         df = pd.read_csv( mypath + file , index_col='timestamp')\n",
    "#         df.index  = pd.to_datetime( df.index )\n",
    "#         df_ffd = pd.DataFrame(index=df.index)\n",
    "\n",
    "#         for col in cols_to_perform_ffd:\n",
    "#             df_ffd[col+\"_0.3\"]  = get_frac_diff_series(df, col, coef_d=0.3, plot=False)\n",
    "#             df_ffd[col+\"_0.6\"]  = get_frac_diff_series(df, col, coef_d=0.6, plot=False)\n",
    "#         max_nans            = df_ffd.isna().sum().max() + 1\n",
    "#         df_ffd              = df_ffd[max_nans:]\n",
    "\n",
    "#         df_ffd.to_csv(file_path)\n",
    "#     except Exception as e:\n",
    "#         print(e, file, \" failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp            = pd.read_csv(mypath + \"A.csv\").set_index('timestamp')[cols_to_perform_ffd]\n",
    "temp.index      = pd.to_datetime(temp.index)\n",
    "# temp            = temp.resample('D', label='right').last().dropna()[['close']]\n",
    "\n",
    "temp            = temp[['close']].pct_change().reset_index().rename({'timestamp': 'date', 'close': 'daily_returns'}, axis=1).dropna()\n",
    "temp            = temp.set_index('date')\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_window_length = 24\n",
    "# ~10 mins to run 750 points (or 2 years at daily granularity)\n",
    "run_module(\n",
    "    temp, lookback_window_length, \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/CPD/day/A.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.689471\n",
       "1       0.548039\n",
       "2       0.834246\n",
       "3       0.819757\n",
       "4       0.917409\n",
       "          ...   \n",
       "2437    0.519940\n",
       "2438    0.605042\n",
       "2439    0.083087\n",
       "2440    0.627954\n",
       "2441    0.473023\n",
       "Name: cp_location_norm, Length: 2442, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv( \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/CPD/day/A.csv\" ).cp_location_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb4adfdb4547441106957b6f6e1e08817e8dd3ceb1745b71e559180c4d2d7141"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
