{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from multiprocessing import Pool, freeze_support\n",
    "from matplotlib import pyplot as plt\n",
    "from changepoint_detection import *\n",
    "from utils import standardize\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from models.losses import *\n",
    "from ts2vec import TS2Vec\n",
    "from tqdm import tqdm\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datautils\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from polygon import RESTClient\n",
    "from metafeatures import *\n",
    "\n",
    " \n",
    "\n",
    "USE_KM_HYP_TO_INITIALISE_KC = True\n",
    "\n",
    "# data_url    = 'https://datahub.io/core/s-and-p-500-companies/datapackage.json'\n",
    "# f           = open(\"/Users/abuj/Documents/GitHub/CFT/datasets/sp500_constituents.json\")\n",
    "# data        = json.load(f)\n",
    "# # len( data['2012/12/31'] )\n",
    "\n",
    "\n",
    "print ( os.cpu_count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH                           = \"/Users/abuj/Documents/GitHub/CFT/datasets/\" #\"/home/ec2-user/CFT/datasets/\"\n",
    "stock_data_path_hour                = BASE_PATH + \"STOCKS/hour/\"\n",
    "ffd_path_hour                       = BASE_PATH + \"META_FEATURES/FFD/hour/\"\n",
    "cpd_path_hour                       = BASE_PATH + \"META_FEATURES/CPD/hour/\"\n",
    "\n",
    "stock_data_path_day                 = BASE_PATH + \"STOCKS/day/\"\n",
    "ffd_path_day                        = BASE_PATH + \"META_FEATURES/FFD/day/\"\n",
    "cpd_path_day                        = BASE_PATH + \"META_FEATURES/CPD/day/\"\n",
    "\n",
    "cpd_path_test                       = BASE_PATH + \"META_FEATURES/CPD/test/\"\n",
    "cpd_path_test2                      = BASE_PATH + \"META_FEATURES/CPD/test2/\"\n",
    "\n",
    "short_cpd_lookback_window_length    = 12\n",
    "long_cpd_lookback_window_length     = 126\n",
    "\n",
    "filenames_hour                      = next(walk(stock_data_path_hour), (None, None, []))[2] \n",
    "filenames_day                       = next(walk(stock_data_path_day), (None, None, []))[2]  \n",
    "\n",
    "cols_to_perform_ffd = ['open', 'high', 'low', 'close', 'vwap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [08:59<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "'''Download data from polygon'''\n",
    "\n",
    "# client = RESTClient(\"ZV6Xy3SWzsJ2xawzRp0JXlmUbbIHePWF\")\n",
    "# START_DATE  = pd.to_datetime(\"2012-12-31\")\n",
    "# END_DATE    = pd.to_datetime(\"2022-10-09\")\n",
    "\n",
    "# with open(\"/Users/abuj/Documents/GitHub/CFT/datasets/ASSETS_NOT_SUPPORTED_BY_POLYGON\", \"rb\") as fp:\n",
    "#    ASSETS_NOT_SUPPORTED_BY_POLYGON = pickle.load(fp)\n",
    "\n",
    "# ASSETS = data['2012/12/31']\n",
    "# ASSETS = set(ASSETS) - set(ASSETS_NOT_SUPPORTED_BY_POLYGON)\n",
    "# ASSETS = list( ASSETS )\n",
    "\n",
    "# aggregates = ['day', 'hour']\n",
    "# time_delta = {}\n",
    "# time_delta['day']  = pd.Timedelta(days=1500)\n",
    "# time_delta['hour'] = pd.Timedelta(hours=800)\n",
    "\n",
    "# failed_assets = []\n",
    "# stocks_folder = \"/Users/abuj/Documents/GitHub/CFT/datasets/STOCKS/\"\n",
    "# for timeframe in aggregates:\n",
    "#     for x in tqdm( ASSETS[:] ):\n",
    "#         if x in ASSETS_NOT_SUPPORTED_BY_POLYGON: continue\n",
    "#         try:\n",
    "#             file_path = stocks_folder + timeframe + \"/\" + x + \".csv\"\n",
    "#             my_file = Path(file_path)\n",
    "#             if my_file.is_file(): continue\n",
    "            \n",
    "#             curr_date = START_DATE\n",
    "#             aggs = []\n",
    "#             while curr_date < END_DATE:\n",
    "#                 until_date = curr_date + time_delta[timeframe]\n",
    "#                 until_date = min(until_date, END_DATE)\n",
    "#                 if (until_date - curr_date).days <= 0:\n",
    "#                     break\n",
    "#                 # print(curr_date, until_date)\n",
    "#                 aggs += client.get_aggs(x, 1, timeframe, curr_date.strftime(\"%Y-%m-%d\"), until_date.strftime(\"%Y-%m-%d\"), limit=50000)\n",
    "#                 curr_date += time_delta[timeframe]\n",
    "\n",
    "#             df = pd.DataFrame(aggs).drop('otc', axis=1)\n",
    "#             df.timestamp = pd.to_datetime(df.timestamp, unit='ms', utc=True)\n",
    "#             df.drop_duplicates(inplace=True)\n",
    "#             df.set_index('timestamp', inplace=True)\n",
    "\n",
    "#             df.to_csv(file_path)\n",
    "#         except:\n",
    "#             print(x, \" failed\")\n",
    "#             failed_assets.append(x)\n",
    "\n",
    "\n",
    "# li = list( set( failed_assets + ASSETS_NOT_SUPPORTED_BY_POLYGON ) )\n",
    "# with open(\"/Users/abuj/Documents/GitHub/CFT/datasets/ASSETS_NOT_SUPPORTED_BY_POLYGON\", \"wb\") as fp:   #Pickling\n",
    "#    pickle.dump(li, fp)\n",
    "\n",
    "'''Process FFD data from polygon'''\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "for file in tqdm(filenames_day[:]):\n",
    "    try:\n",
    "        file_path = ffd_path_day + \"/\" + file\n",
    "        my_file = Path(file_path)\n",
    "        # if my_file.is_file(): continue\n",
    "        \n",
    "        df = pd.read_csv( stock_data_path_day + file , index_col='timestamp')\n",
    "        df.index  = pd.to_datetime( df.index )\n",
    "        df_ffd = pd.DataFrame(index=df.index)\n",
    "\n",
    "        for col in cols_to_perform_ffd:\n",
    "            df_ffd[col+\"_0.3\"]  = get_frac_diff_series(df, col, coef_d=0.3, plot=False)\n",
    "            df_ffd[col+\"_0.6\"]  = get_frac_diff_series(df, col, coef_d=0.6, plot=False)\n",
    "\n",
    "        df_ffd = df_ffd.dropna(axis=1, thresh=int(0.6*df_ffd.shape[0]))\n",
    "\n",
    "        max_nans            = df_ffd.isna().sum().max() + 1\n",
    "        df_ffd              = df_ffd[max_nans:]\n",
    "\n",
    "        if df_ffd.shape[0] < 1800:\n",
    "            print(file, df_ffd.shape)\n",
    "            \n",
    "        df_ffd.to_csv(file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e, file, \" failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/CFT/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/GitHub/CFT/env/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/GitHub/CFT/env/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     temp            \u001b[39m=\u001b[39m temp\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m temp\n\u001b[0;32m----> 9\u001b[0m prep_data_for_cpd(filenames_day[\u001b[39m0\u001b[39;49m], stock_data_path_day)\u001b[39m.\u001b[39;49mhead()[\u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m~/Documents/GitHub/CFT/env/lib/python3.9/site-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3804\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3806\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/GitHub/CFT/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "\n",
    "def prep_data_for_cpd(file, folder_path):\n",
    "\n",
    "    temp            = pd.read_csv(folder_path + file).set_index('timestamp')[cols_to_perform_ffd]\n",
    "    temp.index      = pd.to_datetime(temp.index)\n",
    "    temp            = temp[['close']].pct_change().reset_index().rename({'timestamp': 'date', 'close': 'daily_returns'}, axis=1).dropna()\n",
    "    temp            = temp.set_index('date')\n",
    "    \n",
    "    return temp\n",
    "prep_data_for_cpd(filenames_day[0], stock_data_path_day).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookback_window_length = 24\n",
    "# # ~10 mins to run 750 points (or 2 years at daily granularity) 30 mins to finish daily\n",
    "# run_module(\n",
    "#     temp, lookback_window_length, \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/CPD/day/A.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# files = filenames_day[:]\n",
    "# cpd_args_day_short = [( prep_data_for_cpd(x, stock_data_path_day),  short_cpd_lookback_window_length, cpd_path_day  + x[:-4] + \"_short.csv\" ) for x in files]\n",
    "# cpd_args_day_long  = [( prep_data_for_cpd(x, stock_data_path_day),  long_cpd_lookback_window_length,  cpd_path_day  + x[:-4] + \"_long.csv\"  ) for x in files]\n",
    "\n",
    "# with Pool(48) as pool:\n",
    "#     results = pool.starmap(run_module, cpd_args_day_short)\n",
    "#     results = pool.starmap(run_module, cpd_args_day_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = filenames_hour[:]\n",
    "# cpd_args_hour_short = [( prep_data_for_cpd(x, stock_data_path_hour),  short_cpd_lookback_window_length, cpd_path_hour  + x[:-4] + \"_short.csv\"  ) for x in files]\n",
    "# cpd_args_hour_long  = [( prep_data_for_cpd(x, stock_data_path_hour),  short_cpd_lookback_window_length, cpd_path_hour  + x[:-4] + \"_long.csv\"   ) for x in files]\n",
    "\n",
    "# with Pool() as pool:\n",
    "#     results = pool.starmap(run_module, cpd_args_hour_short)\n",
    "#     results = pool.starmap(run_module, cpd_args_hour_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_consistency_between_two_dataframes(df1, df2):\n",
    "\n",
    "    #TODO Add more verification\n",
    "\n",
    "    for col in df1.columns: assert col in df2.columns\n",
    "    assert df1.shape == df2.shape\n",
    "\n",
    "    for col in df1:\n",
    "        print((df1[col] - df2[col]).mean())\n",
    "        assert (df1[col] - df2[col]).mean() == 0\n",
    "\n",
    "    print(\"The two DataFrames are the same\")\n",
    "\n",
    "    pass\n",
    "\n",
    "df1 = pd.read_csv( cpd_path_test2 +\"/CSCO.csv\" ).set_index('date')\n",
    "\n",
    "df2 = pd.read_csv( cpd_path_day+\"/CSCO.csv\" ).set_index('date')\n",
    "(abs(df1 - df2) > 1e-4).mean()\n",
    "(df1 - df2).cp_location_norm.plot()\n",
    "check_for_consistency_between_two_dataframes(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.689471\n",
       "1       0.548039\n",
       "2       0.834246\n",
       "3       0.819757\n",
       "4       0.917409\n",
       "          ...   \n",
       "2437    0.519940\n",
       "2438    0.605042\n",
       "2439    0.083087\n",
       "2440    0.627954\n",
       "2441    0.473023\n",
       "Name: cp_location_norm, Length: 2442, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv( \"/Users/abuj/Documents/GitHub/CFT/datasets/META_FEATURES/CPD/A_temp_cpd.csv\" ).cp_location_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb4adfdb4547441106957b6f6e1e08817e8dd3ceb1745b71e559180c4d2d7141"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
